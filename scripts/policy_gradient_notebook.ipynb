{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook contains implementaions of a few basic policy gradient algorithms.\n",
    "\n",
    "The implementaions here use non-linear function approximation through Neural Networks.\n",
    "\n",
    "The graph for training loss and agent performance are written to Tensorboard.\n",
    "\n",
    "Have Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pg_agents import REINFORCE, ActorCritic\n",
    "from plots import plot_var_history, ValuePlot_2D, FunctionPlot_3D\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Agents\n",
    "        - Policy Gradient with Baseline\n",
    "        - Actor-Critic\n",
    "\n",
    "## Hyperparameters:\n",
    "\n",
    "### Neural Network:\n",
    "        - Network Architecture\n",
    "        - Learning Rate\n",
    "\n",
    "### Algorithm\n",
    "        - Advantage Function\n",
    "        - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(parameter_dict):\n",
    "    pol_hid_lyrs = ''.join(str(parameter_dict.get(\"pol_hid_lyrs\")).split(','))\n",
    "    val_hid_lyrs = ''.join(str(parameter_dict.get(\"val_hid_lyrs\")).split(','))\n",
    "    pol_lr = parameter_dict.get(\"pol_lr\")\n",
    "    val_lr = parameter_dict.get(\"val_lr\")\n",
    "    batch_size = parameter_dict.get(\"batch_size\")\n",
    "    weight_decay = parameter_dict.get(\"weight_decay\")\n",
    "    name = 'actor_hid_{0}_lr_{1}_critic_hid_{2}_lr_{3:.0e}_wd_{4}_batch_{5}'.format(pol_hid_lyrs, pol_lr, val_hid_lyrs, val_lr, weight_decay, batch_size)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experient(env, num_runs, num_episodes, agent_args, train_writer=None,\n",
    "            render_env = False, plot_value_func = False, plot_state_visit = False):\n",
    "    reward_history = []\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0] if isinstance(env.action_space, gym.spaces.Box) else env.action_space.n\n",
    "    for i, (Agent, agent_kwargs) in enumerate(agent_args):\n",
    "        print(\"Agent Setting {}\".format(i+1))\n",
    "        reward_history.append([])\n",
    "        # Start the runs for each setting\n",
    "        for run in range(1, num_runs+1):\n",
    "            reward_history[i].append([])\n",
    "            env.seed(run)\n",
    "            agent_kwargs[\"seed\"] = run\n",
    "            agent = Agent(state_dim, act_dim, **agent_kwargs)\n",
    "            # Instantiate plot for value function\n",
    "            if plot_value_func:\n",
    "                value_plot = ValuePlot(agent)\n",
    "            # Instantiate plot for state visitation count\n",
    "            if plot_state_visit:\n",
    "                state_freq_plot =  StateFrequencyPlot(agent)\n",
    "            # Start the episodes\n",
    "            for episode in tqdm(range(1, num_episodes+1)):\n",
    "                observation  = env.reset()\n",
    "                done = False\n",
    "                time_step = 0\n",
    "                action = agent.start(observation)\n",
    "                # Start interaction with environment\n",
    "                while True:\n",
    "                    if render_env:\n",
    "                        env.render()\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    time_step +=1\n",
    "                    if done:\n",
    "                        actor_loss, critic_loss = agent.end(reward)\n",
    "                        if train_writer:\n",
    "                            writer.add_scalars('Run {}'.format(run), {'Actor Loss':actor_loss,\n",
    "                                    'Critic Loss':critic_loss, 'Total Timesteps': time_step}, episode)\n",
    "                        break\n",
    "                    else:\n",
    "                        action = agent.take_step(reward, observation)\n",
    "                    # Update state visits\n",
    "                    if plot_state_visit:\n",
    "                        pos = int((observation[0]-state_freq_plot.left_limit[0]) / state_freq_plot.steps[0])\n",
    "                        vel = int((observation[1]-state_freq_plot.left_limit[1]) / state_freq_plot.steps[1])\n",
    "                        state_freq_plot.visits[vel, pos] +=1\n",
    "                        if (episode in [1, 10, 25, 100, 500, 1000, 200, 3500, 5000] and done):\n",
    "                            state_freq_plot.update(\"after {} episodes\".format(episode)) \n",
    "                    # Plot the value function at fixed intervals\n",
    "                    if plot_value_func:\n",
    "                        if (time_step % 200 == 0 and episode == 1):\n",
    "                            value_plot.update(\"after {} timesteps\".format(time_step))\n",
    "                        elif (episode in [10, 50, 100, 200, 500, 1000, 2000, 3500, 5000] and done):\n",
    "                            value_plot.update(\"after {} episodes\".format(episode))\n",
    "                reward_history[i][run-1].append(time_step)\n",
    "    return reward_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Cart Pole environment\n",
    "cart_pole_env = gym.make('CartPole-v0').env\n",
    "\n",
    "# Experiment Settting\n",
    "settings = [(\n",
    "             REINFORCE,\n",
    "             {'pol_hid_lyrs': [32], 'val_hid_lyrs': [16],\n",
    "              'pol_lr': 0.001, 'val_lr': 0.005, 'batch_size': 5000, 'is_discrete': True}\n",
    "              )]\n",
    "\n",
    "\n",
    "model_name = 'actor_hid_[32]_lr_1e-3_critic_hid_[16]_lr_5e-3_batch_5000'\n",
    "\n",
    "# Setup Tensorboard path \n",
    "writer = SummaryWriter('../runs/' + Environment + '/REINFORCE/' + model_name)\n",
    "\n",
    "\n",
    "_, _ = run_experient(cart_pole_env, 1, 5000, settings, render_env= True, train_writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Mountain Car environment\n",
    "Environment = 'MountainCarContinuous-v0'\n",
    "mountain_car_env = gym.make(Environment).env\n",
    "\n",
    "# Experiment Settting\n",
    "settings = [(\n",
    "             REINFORCE,\n",
    "             {'pol_hid_lyrs': [16, 32, 32, 32, 16], 'val_hid_lyrs': [16, 32, 32, 16],\n",
    "              'pol_lr': 5e-4, 'val_lr': 5e-3, 'weight_decay':0.01, 'batch_size': 500, 'is_discrete': False}\n",
    "              )]\n",
    "\n",
    "model_name = get_model_name(settings[0][1])\n",
    "\n",
    "# Setup Tensorboard path \n",
    "writer = SummaryWriter('../runs/' + Environment + '/REINFORCE/' + model_name)\n",
    "\n",
    "\n",
    "rew_hist, ann_agent = run_experient(mountain_car_env, 1, 5000, settings, train_writer=writer, render_env=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit91f1a17592f44d86b516356def5c0b00",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}