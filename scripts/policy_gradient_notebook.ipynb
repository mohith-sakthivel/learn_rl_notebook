{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This Notebook contains implementaions of a few basic policy gradient algorithms.\n",
    "\n",
    "The implementaions here use non-linear function approximation through Neural Networks.\n",
    "\n",
    "The graph for training loss and agent performance are written to Tensorboard.\n",
    "\n",
    "Have Fun!\n",
    "\n",
    "#### Note: Unlike the tabular linear function approximation texhniques seen earlier, the hyperparameters are more sensitive. Thus a good amount of tuning is required to get a decent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pg_agents import REINFORCE, REINFORCE_Baseline, ActorCritic\n",
    "from plots import plot_var_history\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Agents\n",
    "        - REINFORCE \n",
    "        - REINFORCE with Baseline\n",
    "        - Actor-Critic\n",
    "\n",
    "## Hyperparameters:\n",
    "\n",
    "### Neural Network:\n",
    "        - Network Architecture\n",
    "        - Learning Rate\n",
    "        - Activation\n",
    "\n",
    "### Algorithm\n",
    "        - Advantage Function\n",
    "        - Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(parameter_dict , critic=False):\n",
    "    pol_hid_lyrs = ''.join(str(parameter_dict.get(\"pol_hid_lyrs\")).split(','))\n",
    "    pol_lr = parameter_dict.get(\"pol_lr\")\n",
    "    decay = parameter_dict.get(\"lr_decay\", 1)\n",
    "    if critic:\n",
    "        val_hid_lyrs = ''.join(str(parameter_dict.get(\"val_hid_lyrs\")).split(','))\n",
    "        val_lr = parameter_dict.get(\"val_lr\")\n",
    "    batch_size = parameter_dict.get(\"batch_size\")\n",
    "    if not critic:\n",
    "        name = 'actor_hid_{0}_lr_{1}_decay_{2}_batch_{3}'.format(pol_hid_lyrs, pol_lr, decay, batch_size)\n",
    "    else:\n",
    "        name = 'actor_hid_{0}_lr_{1}_critic_hid_{2}_lr_{3:.0e}_decay_{4}_batch_{5}'.format(pol_hid_lyrs, pol_lr, val_hid_lyrs, val_lr, decay, batch_size)\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experient(env, num_runs, num_episodes, agent_args, tb_path=None,\n",
    "            render_env = False, plot_value_func = False, plot_state_visit = False):\n",
    "    reward_history = []\n",
    "    agents_hist = []\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0] if isinstance(env.action_space, gym.spaces.Box) else env.action_space.n\n",
    "    for i, (Agent, agent_kwargs) in enumerate(agent_args):\n",
    "        reward_history.append([])\n",
    "        # Start the runs for each setting\n",
    "        for run in tqdm(range(1, num_runs+1), desc=\"Parameter Setting {}\".format(i+1)):\n",
    "            reward_history[i].append([])\n",
    "            env.seed(run)\n",
    "            epoch = 0\n",
    "            agent_kwargs[\"seed\"] = run\n",
    "            agent = Agent(state_dim, act_dim, **agent_kwargs)\n",
    "            if run == 1 and tb_path:\n",
    "                model_name = get_model_name(agent_kwargs, critic=agent.baseline)\n",
    "                writer = SummaryWriter(tb_path + '/' + agent.__str__() + '/' + model_name)\n",
    "            # Start the episodes\n",
    "            for episode in range(1, num_episodes+1):\n",
    "                observation  = env.reset()\n",
    "                done = False\n",
    "                time_step = 0\n",
    "                action = agent.start(observation)\n",
    "                # Start interaction with environment\n",
    "                while not done:\n",
    "                    if render_env:\n",
    "                        env.render()\n",
    "                    # Take a step in the environment\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    time_step +=1\n",
    "                    if not done:\n",
    "                        # Get next action from agent\n",
    "                        action = agent.take_step(reward, observation)\n",
    "                    else:\n",
    "                        episode_reward, actor_loss, critic_loss = agent.end(reward)\n",
    "                        reward_history[i][run-1].append(episode_reward)\n",
    "                        if run == 1 and tb_path:\n",
    "                            writer.add_scalar('Reward', episode_reward, episode)\n",
    "                            if actor_loss:\n",
    "                                epoch += 1\n",
    "                                writer.add_scalar('Actor Loss', actor_loss, epoch)\n",
    "                                writer.add_scalar('Episode Number', episode, epoch)\n",
    "                                if critic_loss:\n",
    "                                    writer.add_scalar('Critic Loss', critic_loss, epoch)\n",
    "            if run == 1:\n",
    "                agents_hist.append(agent)\n",
    "                if tb_path:\n",
    "                    writer.close()\n",
    "    env.close()\n",
    "    return reward_history, agents_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Cart Pole environment\n",
    "Environment = 'CartPole-v1'\n",
    "cart_pole_env = gym.make(Environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantage of Baseline:\n",
    "\n",
    "The below experiment runs REINFORCE algorithm with and without Baseline.\n",
    "\n",
    "Using a baseline reduces the variance and accelrates the convergence.\n",
    "\n",
    "Note that the neural network used to approximate the value fuction must be big enough to represent it efficiently. Using a very small network causes slower learning and may converge to a cruder local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Experiment Settting\n",
    "settings = [(REINFORCE, {'pol_hid_lyrs': [16, 16], 'lr_decay': 0.99, 'pol_lr': 0.025,\n",
    "                         'min_pol_lr':0.0025, 'batch_size': 5000, 'is_discrete': True}),\n",
    "            (REINFORCE_Baseline,\n",
    "             {'pol_hid_lyrs': [16, 16], 'val_hid_lyrs': [16, 32, 16], 'lr_decay': 0.99,\n",
    "              'pol_lr': 0.025, 'min_pol_lr': 0.0025, 'val_lr': 0.05, 'min_val_lr': 0.005, \n",
    "              'batch_size': 5000, 'is_discrete': True})]\n",
    "\n",
    "# Setup Tensorboard path \n",
    "writer_path = '../runs/' + Environment\n",
    "\n",
    "reward_hist, agent_hist = run_experient(cart_pole_env, 5, 3000, settings, render_env=False, tb_path=writer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Results\n",
    "plot_args = {'x_label': 'Episode', 'y_label': 'Reward per Episode\\n(Averaged over 5 runs)',\n",
    "            'log_scale': False, 'y_ticks': [25, 50, 100, 200, 300, 400, 500]}\n",
    "\n",
    "labels = [\"REINFORCE\", \"REINFORCE with Baseline\"]\n",
    "\n",
    "plot_var_history(reward_hist, labels, **plot_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Parameter Setting 1: 100%|██████████| 5/5 [02:05<00:00, 25.03s/it]\n"
    }
   ],
   "source": [
    "# Experiment Settting\n",
    "settings = [(ActorCritic,\n",
    "             {'pol_hid_lyrs': [16, 16], 'val_hid_lyrs': [16, 32, 16], 'lr_decay': 0.99,\n",
    "              'pol_lr': 0.01, 'min_pol_lr': 0.0025, 'val_lr': 0.025, 'min_val_lr': 0.005, \n",
    "              'batch_size': 1000, 'is_discrete': True, 'discount': 0.99})]\n",
    "\n",
    "# Setup Tensorboard path \n",
    "writer_path = '../runs/' + Environment\n",
    "\n",
    "reward_hist, agent_hist = run_experient(cart_pole_env, 5, 3000, settings, render_env=False, tb_path=writer_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit91f1a17592f44d86b516356def5c0b00",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}