{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "\n",
    "from neural_agents import REINFORCE, ActorCritic\n",
    "from plots import plot_var_history, ValuePlot_2D, FunctionPlot_3D\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experient(env, num_runs, num_episodes, agent_args, train_writer=None,\n",
    "            render_env = False, plot_value_func = False, plot_state_visit = False):\n",
    "    reward_history = []\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0] if isinstance(env.action_space, gym.spaces.Box) else env.action_space.n\n",
    "    for i, (Agent, agent_kwargs) in enumerate(agent_args):\n",
    "        print(\"Agent Setting {}\".format(i+1))\n",
    "        reward_history.append([])\n",
    "        # Start the runs for each setting\n",
    "        for run in range(1, num_runs+1):\n",
    "            reward_history[i].append([])\n",
    "            env.seed(run)\n",
    "            agent_kwargs[\"seed\"] = run\n",
    "            agent = Agent(state_dim, act_dim, **agent_kwargs)\n",
    "            # Instantiate plot for value function\n",
    "            if plot_value_func:\n",
    "                value_plot = ValuePlot(agent)\n",
    "            # Instantiate plot for state visitation count\n",
    "            if plot_state_visit:\n",
    "                state_freq_plot =  StateFrequencyPlot(agent)\n",
    "            # Start the episodes\n",
    "            for episode in tqdm(range(1, num_episodes+1)):\n",
    "                observation  = env.reset()\n",
    "                done = False\n",
    "                time_step = 0\n",
    "                action = agent.start(observation)\n",
    "                # Start interaction with environment\n",
    "                while True:\n",
    "                    if render_env:\n",
    "                        env.render()\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    time_step +=1\n",
    "                    if done:\n",
    "                        actor_loss, critic_loss = agent.end(reward)\n",
    "                        if train_writer:\n",
    "                            writer.add_scalars('Run {}'.format(run), {'Actor Loss':actor_loss,\n",
    "                                    'Critic Loss':critic_loss, 'Total Timesteps': time_step}, episode)\n",
    "                        break\n",
    "                    else:\n",
    "                        action = agent.take_step(reward, observation)\n",
    "                    # Update state visits\n",
    "                    if plot_state_visit:\n",
    "                        pos = int((observation[0]-state_freq_plot.left_limit[0]) / state_freq_plot.steps[0])\n",
    "                        vel = int((observation[1]-state_freq_plot.left_limit[1]) / state_freq_plot.steps[1])\n",
    "                        state_freq_plot.visits[vel, pos] +=1\n",
    "                        if (episode in [1, 10, 25, 100, 500, 1000, 200, 3500, 5000] and done):\n",
    "                            state_freq_plot.update(\"after {} episodes\".format(episode)) \n",
    "                    # Plot the value function at fixed intervals\n",
    "                    if plot_value_func:\n",
    "                        if (time_step % 200 == 0 and episode == 1):\n",
    "                            value_plot.update(\"after {} timesteps\".format(time_step))\n",
    "                        elif (episode in [10, 50, 100, 200, 500, 1000, 2000, 3500, 5000] and done):\n",
    "                            value_plot.update(\"after {} episodes\".format(episode))\n",
    "                reward_history[i][run-1].append(time_step)\n",
    "    return reward_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Cart Pole environment\n",
    "# cart_pole_env = gym.make('CartPole-v0').env\n",
    "\n",
    "# # Experiment Settting\n",
    "# settings = [(\n",
    "#              REINFORCE,\n",
    "#              {'pol_hid_lyrs': [16], 'val_hid_lyrs': [16],\n",
    "#               'pol_lr': 0.001, 'val_lr': 0.005, 'batch_size': 5000, 'is_discrete': True}\n",
    "#               )]\n",
    "\n",
    "# # Setup Tensorboard path \n",
    "# writer = SummaryWriter('../runs/CartPole-v0/REINFORCE')\n",
    "\n",
    "\n",
    "# _, _ = run_experient(cart_pole_env, 1, 5000, settings, train_writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Mountain Car environment\n",
    "# Environment = 'MountainCarContinuous-v0'\n",
    "# mountain_car_env = gym.make(Environment).env\n",
    "\n",
    "# # Experiment Settting\n",
    "# settings = [(\n",
    "#              REINFORCE,\n",
    "#              {'pol_hid_lyrs': [16, 32, 16], 'val_hid_lyrs': [16, 16],\n",
    "#               'pol_lr': 1e-4, 'val_lr': 1e-3, 'batch_size': 500, 'is_discrete': False}\n",
    "#               )]\n",
    "\n",
    "# model_name = 'actor_hid_[16 32 16]_lr_1e-4_critic_hid_[16 16]_lr_1e-3_batch_500'\n",
    "\n",
    "# # Setup Tensorboard path \n",
    "# writer = SummaryWriter('../runs/' + Environment + '/REINFORCE/' + model_name)\n",
    "\n",
    "\n",
    "# rew_hist, ann_agent = run_experient(mountain_car_env, 1, 5000, settings, train_writer=writer, render_env=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Agent Setting 1\n  0%|          | 2/5000 [00:13<7:59:12,  5.75s/it]"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2ae9ab0e4d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mrew_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountain_car_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-c315f91b531d>\u001b[0m in \u001b[0;36mrun_experient\u001b[0;34m(env, num_runs, num_episodes, agent_args, train_writer, render_env, plot_value_func, plot_state_visit)\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                     \u001b[0;31m# Update state visits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mplot_state_visit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/learn_rl_notebook/scripts/neural_agents.py\u001b[0m in \u001b[0;36mtake_step\u001b[0;34m(self, reward, state)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         state_tensor = torch.as_tensor(\n\u001b[0;32m--> 149\u001b[0;31m                             state, device=self.device, dtype=torch.float32)\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup Mountain Car environment\n",
    "Environment = 'MountainCarContinuous-v0'\n",
    "mountain_car_env = gym.make(Environment).env\n",
    "\n",
    "# Experiment Settting\n",
    "settings = [(\n",
    "             ActorCritic,\n",
    "             {'pol_hid_lyrs': [16, 32, 16], 'val_hid_lyrs': [16, 16],\n",
    "              'pol_act': nn.Tanh, 'val_act': nn.Tanh, 'discount': 1,\n",
    "              'pol_lr': 1e-4, 'val_lr': 1e-3, 'batch_size': 500, 'is_discrete': False}\n",
    "              )]\n",
    "\n",
    "model_name = 'actor_hid_[16 32 16]_lr_1e-4_critic_hid_[16 16]_lr_1e-3_batch_500'\n",
    "\n",
    "# Setup Tensorboard path \n",
    "writer = SummaryWriter('../runs/' + Environment + '/Trial/' + model_name)\n",
    "\n",
    "\n",
    "rew_hist, ann_agent = run_experient(mountain_car_env, 1, 5000, settings, train_writer=writer, render_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.5675, -0.6354],\n        [ 0.6367,  2.2053],\n        [-0.4758,  1.8802],\n        [-0.5544, -2.2358],\n        [-0.4321, -1.6297],\n        [-0.0800, -0.0044],\n        [ 1.4590, -0.4275],\n        [ 0.3299, -0.8522],\n        [ 0.1406, -0.2824],\n        [ 0.6118,  2.0799]])\n"
    }
   ],
   "source": [
    "a = torch.randn(10,2)\n",
    "print(a)\n",
    "a,b = torch.split(a,1,dim=1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bit91f1a17592f44d86b516356def5c0b00",
   "display_name": "Python 3.7.7 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}