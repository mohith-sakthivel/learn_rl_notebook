{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar-v0\n",
    "\n",
    "This repository contains a highly optimized implementation of the MountainCar-v0 problem.\n",
    "\n",
    "Please checkout the [openai]( https://gym.openai.com/envs/MountainCar-v0/) website and ther [github](https://github.com/openai/gym/wiki/) repo for more details on the MountainCar-v0 environmnet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from utils import TileEncoder, plot_var_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TO_SARSA_Lambda_agent():\n",
    "    \"\"\"\n",
    "    Agent that learns using True Online SARSA (Lambda) Agent\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_limits, num_actions, epsilon=0.1, step_size=0.5, lambda_val=0.95,\n",
    "                 discount=1, seed=None, decay_factor=None, num_tilings=8, num_tiles=None):\n",
    "        # set seed if provided\n",
    "        self.seed = seed\n",
    "        self.policy_rand_generator = np.random.default_rng(self.seed)\n",
    "        # setup tile encoding\n",
    "        if num_tiles == None:\n",
    "            num_tiles = [8] * len(obs_limits)\n",
    "        self.tc = TileEncoder(obs_limits, num_tiles, num_tilings)\n",
    "        # set agent parameters\n",
    "        self.num_actions = num_actions\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "        self.step_size = step_size/self.tc.num_tilings\n",
    "        self.d_step_size = self.step_size\n",
    "        self.decay_factor = decay_factor\n",
    "        self.value_function = np.zeros((self.num_actions, self.tc.iht_size, self.tc.num_tilings),\n",
    "                                       dtype=np.float32)\n",
    "        self.indices = [i for i in range(self.tc.num_tilings)]\n",
    "        self.episode = 0\n",
    "        # create experience buffers\n",
    "        self.state_buffer = deque()\n",
    "        self.action_buffer = deque()\n",
    "        self.reward_buffer = deque()\n",
    "        self.time_step = 0\n",
    "        self.lambda_val = lambda_val\n",
    "\n",
    "    def agent_policy(self, state):\n",
    "        \"\"\"\n",
    "        Return an action according to the policy given a state\n",
    "        \"\"\"\n",
    "        if self.policy_rand_generator.random() < self.epsilon:\n",
    "            # Take random exploratory action\n",
    "            return self.policy_rand_generator.integers(0, self.num_actions)\n",
    "        else:\n",
    "            # Take greedy action w.r.to current value function\n",
    "            action_vals= np.sum(self.value_function[:, self.tc.get_feature(state), self.indices], axis=1)\n",
    "            max_val = np.amax(action_vals)\n",
    "            max_actions = np.where(action_vals == max_val)[0]\n",
    "            return self.policy_rand_generator.choice(max_actions)\n",
    "\n",
    "    def start(self, state):\n",
    "        \"\"\" Start the agent for the episode \"\"\"\n",
    "        self.episode += 1\n",
    "        self.state_buffer.append(state)\n",
    "        self.action_buffer.append(self.agent_policy(state))\n",
    "        self.trace = np.zeros_like(self.value_function, dtype=np.float32)\n",
    "        self.Q_Old = 0\n",
    "        return self.action_buffer[0]\n",
    "    \n",
    "    def update_value(self, terminal):\n",
    "        \"\"\"Performs an update of the value funciton\"\"\"\n",
    "        # Calculate TD error\n",
    "        fv = self.tc.get_feature(self.state_buffer[0])\n",
    "        Q = np.sum(self.value_function[self.action_buffer[0], fv, self.indices])\n",
    "        if terminal:\n",
    "            Q_dash = 0\n",
    "        else:\n",
    "            fv_dash = self.tc.get_feature(self.state_buffer[1])\n",
    "            Q_dash = np.sum(self.value_function[self.action_buffer[1], fv_dash, self.indices])\n",
    "        td_error = (self.reward_buffer[0] + self.discount * Q_dash) - Q\n",
    "        # Estimate Dutch's trace\n",
    "        multiplier = self.step_size * self.discount * self.lambda_val\n",
    "        multiplier = multiplier * np.sum(self.trace[self.action_buffer[0], fv, self.indices])\n",
    "        self.trace *= self.lambda_val * self.discount\n",
    "        self.trace[self.action_buffer[0], fv, self.indices] += (1 - multiplier)\n",
    "        # Update value function\n",
    "        self.value_function += self.step_size * (td_error + Q - self.Q_Old) * self.trace\n",
    "        self.value_function[self.action_buffer[0], fv, self.indices] -= self.step_size * (Q-self.Q_Old)\n",
    "        self.Q_Old = Q_dash\n",
    "        self.state_buffer.popleft()\n",
    "        self.action_buffer.popleft()\n",
    "        self.reward_buffer.popleft()\n",
    "\n",
    "    def take_step(self, reward, state):\n",
    "        \"\"\"\n",
    "        Agent updates the state values and returns the next action\n",
    "        \"\"\"\n",
    "        self.time_step += 1\n",
    "        action = self.agent_policy(state)\n",
    "        self.state_buffer.append(state)\n",
    "        self.action_buffer.append(action)\n",
    "        self.reward_buffer.append(reward)\n",
    "        # Perform value function update if enough experience is available\n",
    "        self.update_value(terminal=False)\n",
    "        return action\n",
    "\n",
    "    def end(self, reward):\n",
    "        \"\"\"\n",
    "        Terminate the episode and update the values\n",
    "        \"\"\"\n",
    "        self.time_step += 1\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.update_value(terminal=True)\n",
    "        self.reset_episode()\n",
    "\n",
    "    def reset_episode(self):\n",
    "        \"\"\" Resets episode related parameters of the agent \"\"\"\n",
    "        self.state_buffer.clear()\n",
    "        self.action_buffer.clear()\n",
    "        self.reward_buffer.clear()\n",
    "        self.time_step = 0\n",
    "        if self.decay_factor:\n",
    "            self.d_step_size = self.d_step_size * self.decay_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experient(env, num_runs, num_episodes, settings,render_env = False):\n",
    "    num_actions = env.action_space.n\n",
    "    obs_limits = np.transpose(np.vstack((env.observation_space.low, env.observation_space.high)))\n",
    "    reward_history = []\n",
    "    for i, (Agent, agent_args) in enumerate(settings):\n",
    "        time.sleep(1)\n",
    "        reward_history.append([])\n",
    "        # Start the runs for each setting\n",
    "        for run in tqdm(range(1, num_runs+1), desc=\"Parameter Setting {}\".format(i+1)):\n",
    "            reward_history[i].append([])\n",
    "            env.seed(run)\n",
    "            agent_args[\"seed\"] = run\n",
    "            agent = Agent(obs_limits, num_actions, **agent_args)\n",
    "            # Start the episodes\n",
    "            for episode in range(1, num_episodes+1):\n",
    "                observation  = env.reset()\n",
    "                done = False\n",
    "                time_step = 0\n",
    "                action = agent.start(observation)\n",
    "                # Start interaction with environment\n",
    "                while not done:\n",
    "                    if render_env:\n",
    "                        env.render()\n",
    "                        time.sleep(0.001)\n",
    "                    observation, reward, done, info = env.step(action)\n",
    "                    time_step +=1\n",
    "                    if done:\n",
    "                        agent.end(reward)\n",
    "                    else:\n",
    "                        action = agent.take_step(reward, observation)\n",
    "                reward_history[i][run-1].append(time_step)\n",
    "    env.close()\n",
    "    return reward_history, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup environment\n",
    "env = gym.make('MountainCar-v0').env\n",
    "# Set plot parameters\n",
    "plot_args = {'x_label': 'Episode', 'y_label': 'Steps per Episode (log scale)\\n(Averaged over 50 runs)',\n",
    "            'log_scale': True, 'y_ticks': [110, 125, 150, 200, 500, 1000]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Description:\n",
    "\n",
    "The agent runs on the True Online SARSA (lambda) algorithm. The agent uses eligibility traces (Dutch's trace) which is a powerful technique to make more directed updates. The agent keeps track of which components of the function approximator contributed to the error in the value estimates. The trace fades away as the agent moves to successive states in the trajectory. \n",
    "\n",
    "Another interersting fact is that the agent here looks backwards instead of the usual case where it makes updates by looking forward.\n",
    "\n",
    "### Hyperparameters:\n",
    "\n",
    "    - Algorithm: True Online SARSA (lambda) algorithm\n",
    "    - Step Size - 0.1 (Step size decays exponentially with a factor of 0.99 every timestep)\n",
    "    - lambda - 0.9\n",
    "    - epsilon - 0.001\n",
    "    - Tile Encoder - 16 Tilings with 8 tiles per state space dimension\n",
    "    - Discount - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = [(TO_SARSA_Lambda_agent, {\"step_size\": 0.1, \"epsilon\":0.001, \"lambda_val\": 0.9, \"decay_factor\": 0.99, \"num_tilings\": 16})]\n",
    "\n",
    "plot_args['y_label'] = 'Steps per Episode (log scale)\\n(Averaged over 100 runs)'\n",
    "plot_labels = [\"True Online Sarsa (lamda) Agent\"]\n",
    "\n",
    "rewards_t, agent_t = run_experient(env, 100, 500, settings)\n",
    "\n",
    "plot_var_history(rewards_t, plot_labels, **plot_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: For further study on eligibility traces, turn to the Chapter-12 of Prof. Sutton's book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit91f1a17592f44d86b516356def5c0b00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}